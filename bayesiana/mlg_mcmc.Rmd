---
title: "MCMC MLG"
author: "Eduardo Freire, Luis Henrique"
date: "27/07/2022"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
```


```{r}
library(readr)
library(dplyr)
library(ggplot2)
library(broom)
library(kableExtra)
library(coda)
library(forecast)
require(gridExtra)
set.seed(20201011)
```

## Sumário

- Presença de doença cardíaca ou caso de infarto: variável binária(0 ou 1).

- Uso de cigarro - “Você fumou pelo menos 100 cigarros em toda a sua vida?” -  : variável binária(0 ou 1).

- Atividade física - “Adultos que relataram fazer atividade física ou exercício durante os últimos 30 dias além do seu trabalho regular” - : variável binária(0 ou 1).


## Tabela

```{r}
heart_disease <- read_csv("heart_disease.csv") %>% 
    select(HeartDiseaseorAttack, Smoker, PhysActivity) %>% 
    sample_n(5000)
names(heart_disease) <- c("doenca_ou_infarto", "fumante", "ativ_fisica")
kable(head(heart_disease, n=5)) %>% 
  kable_styling(full_width = F)
```

## Prática de exercícios físicos e consumo de cigarro


```{r fig.height=3.5, fig.width=4.5}

par(mfrow=c(1, 2))
kable(table(heart_disease$fumante), caption = "Consumo de cigarro") %>% 
  kable_styling(full_width = F)

kable(table(heart_disease$ativ_fisica), caption = "Prática de atividades físicas") %>% 
  kable_styling(full_width = F)
```

## Modelo proposto

$$ln\Big(\frac{\pi_i}{1-\pi_i}\Big) = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i}$$

onde: $[y_i|\pi_i] \sim Ber(\pi_i), i=1,2,..,n$

e também: 

- $X_{1i}$ : consumo de cigarro do entrevistado i
- $X_{2i}$ : prática de atividade física do entrevistado i

## Prioris normais pouco informativas para os Betas.


Podemos usar normais pouco informativas de duas formas, encontrando a priori de Jeffreys ou assumindo uma variância grande o suficiente para dizer que a normal $N(0, K)$ é pouco informativa.


Para obter a priori de Jeffreys basta saber que a informação de Fisher para uma amostra da normal com $\sigma$ conhecido é dada por:

$$I(\theta)= \frac{n}{\sigma^2}$$
Logo, a priori de Jeffreys é dada por:

$$p(\theta) \propto \frac{\sqrt n}{\sigma} \propto 1$$

Que é uma distribuição imprópria(Uniforme na reta). Logo, poderíamos ter que:

$$p(\beta_i) \propto 1, i=0, 1, 2$$

Porém é suficiente assumir K suficientemente grande e tomar as prioris da forma:


$$p(\beta_i) \sim N(0, K), i=0,1, 2$$

## Condicionais completas


- Para $\beta_0$

$$p(\beta_1, \beta_2, \bf{y}) \propto (\beta_0, \beta_1, \beta_2;\bf{y})$$

$$ \propto (\bf{y}|\beta_0, \beta_1, \beta_2) p(\beta_0) $$


$$ \propto e^{\beta_0 \sum_{i=1}^{n}y_i}\prod_{i=1}^{n}[1+e^{\beta_0+\beta_1x_{1i}+\beta_2x_{2i}}]^{-1} e^{-\beta_0^2/2K}$$

- Para $\beta_1$

$$p(\beta_0, \beta_2, \bf{y}) \propto (\beta_0, \beta_1, \beta_2;\bf{y})$$

$$ \propto (\bf{y}|\beta_0, \beta_1, \beta_2) p(\beta_1) $$


$$ \propto e^{\beta_1 \sum_{i=1}^{n}y_i}\prod_{i=1}^{n}[1+e^{\beta_0+\beta_1x_{1i}+\beta_2x_{2i}}]^{-1} e^{-\beta_1^2/2K}$$

- Para $\beta_2$

$$p(\beta_0, \beta_1, \bf{y}) \propto (\beta_0, \beta_1, \beta_2;\bf{y})$$

$$ \propto (\bf{y}|\beta_0, \beta_1, \beta_2) p(\beta_2) $$


$$ \propto e^{\beta_2 \sum_{i=1}^{n}y_i}\prod_{i=1}^{n}[1+e^{\beta_0+\beta_1x_{1i}+\beta_2x_{2i}}]^{-1} e^{-\beta_2^2/2K}$$

## Funções auxiliares para calcular Metropolis-Hastings

```{r}
calcula_razao_metropolis <- function(x, y, beta, beta_proposto, indice_beta, i, K){

  soma_numerador <- switch(
        indice_beta,
        beta_proposto+beta[[2]][i-1]*x[[1]]+beta[[3]][i-1]*x[[2]],
        beta[[1]][i]+beta_proposto*x[[1]]+beta[[3]][i-1]*x[[2]],
        beta[[1]][i]+beta[[2]][i]*x[[1]]+beta_proposto*x[[2]]
        )
  
  numerador <- sum(dbinom(y,1,exp(soma_numerador)/
                            (1+exp(soma_numerador)),log = TRUE)) +
  dnorm(beta_proposto,0,sqrt(K),log = TRUE)
  
  soma_denominador <- switch(
        indice_beta,
        beta[[1]][i-1]+beta[[2]][i-1]*x[[1]]+beta[[3]][i-1]*x[[2]],
        beta[[1]][i]+beta[[2]][i-1]*x[[1]]+beta[[3]][i-1]*x[[2]],
        beta[[1]][i]+beta[[2]][i]*x[[1]]+beta[[3]][i-1]*x[[2]]
        )
  
  denominador <- sum(dbinom(y,1,exp(soma_denominador)/
                              (1+exp(soma_denominador)),log = TRUE)) +
  dnorm(beta[[indice_beta]][i-1],0,sqrt(K),log = TRUE)

  return(numerador - denominador)
}

analisa_rejeicao_beta <- function(i, beta, beta_proposto, contador, indice_beta, razao_metropolis){
  alpha <- min(0, razao_metropolis)
  gerado <- runif(1,0,1)
  if (alpha >= log(gerado)){
    beta[[indice_beta]][i] <- beta_proposto
    contador[[indice_beta]] <- contador[[indice_beta]] + 1
  } else {
    beta[[indice_beta]][i] <- beta[[indice_beta]][i-1]
  }
  
  return(list(beta, contador))
}
```



## Hiperparâmetros das prioris

```{r}
iteracoes <- 10000
K <- 10000
beta <- list(5, 0.8, 1.3)
variancia <- list(0.058, 0.074, 0.096)
contador <- list(0,0,0)
```

```{r}
y <- heart_disease$doenca_ou_infarto
x <- list(heart_disease$ativ_fisica, heart_disease$fumante)
```

## MCMC

```{r}
for (i in 2:iteracoes){
  for (j in 1:3){
    beta_j_proposto <- rnorm(1, beta[[j]][i-1],sqrt(variancia[[j]]))
    razao_metropolis <- calcula_razao_metropolis(
      x=x, 
      y=y, 
      beta=beta, 
      beta_proposto=beta_j_proposto, 
      indice_beta=j, 
      i=i, 
      K=K)
    
    resultado <- analisa_rejeicao_beta(
      i=i, 
      beta=beta,
      contador=contador,
      beta_proposto=beta_j_proposto, 
      indice_beta=j,
      razao_metropolis=razao_metropolis
    )
    
    beta <- resultado[[1]]
    contador <- resultado[[2]]
  }
}
```

## Taxa de aceitação

```{r}
taxa_aceitacao <- c()
betas <- (0:2)
for(j in 1:3){
  taxa_aceitacao <- c(taxa_aceitacao, contador[[j]]/iteracoes)
}

kable(data.frame(aceitacao=taxa_aceitacao, beta=betas), caption = "Taxa de aceitação dos betas.") %>% 
  kable_styling(full_width = F)
```

## Convergência

```{r}
betas_df <- data.frame(beta0=beta[[1]], beta1=beta[[2]], beta2=beta[[3]])

plot1 <- betas_df %>% 
  ggplot(aes(y=beta0, x=1:iteracoes)) + geom_line(color='slateblue') +
  labs(x="iterações", y=expression(beta[0]))

plot2 <- betas_df %>% 
  ggplot(aes(y=beta1, x=1:iteracoes)) + geom_line(color='#374d7c') +
  labs(x="iterações", y=expression(beta[1]))

plot3 <- betas_df %>% 
  ggplot(aes(y=beta2, x=1:iteracoes)) + geom_line(color='#bf9000') +
  labs(x="iterações", y=expression(beta[2]))

grid.arrange(plot1, plot2, plot3, ncol=3)
```


### Burn-in ideal
 
 
```{r}
burn=100

plot1 <- betas_df[burn:iteracoes,] %>% 
  ggplot(aes(y=beta0, x=burn:iteracoes)) + geom_line(color='slateblue') +
  labs(x="iterações", y=expression(beta[0]))

plot2 <- betas_df[burn:iteracoes,] %>% 
  ggplot(aes(y=beta0, x=burn:iteracoes)) + geom_line(color='#374d7c') +
  labs(x="iterações", y=expression(beta[1]))

plot3 <- betas_df[burn:iteracoes,] %>% 
  ggplot(aes(y=beta0, x=burn:iteracoes)) + geom_line(color='#bf9000') +
  labs(x="iterações", y=expression(beta[2]))


grid.arrange(plot1, plot2, plot3, ncol=3)
```

## Estatísticas após burnin

```{r}
media_burn <- c()
mediana_burn <- c()
moda_burn <- c()
betas <- (0:2)
moda <- function(x) {
  return(unique(x)[which.max(tabulate(match(x, unique(x))))])
}

for(j in 1:3){
  media_burn <- c(media_burn, mean(beta[[j]][burn:iteracoes]))
  mediana_burn <- c(mediana_burn, median(beta[[j]][burn:iteracoes]))
  moda_burn <- c(moda_burn, moda(beta[[j]][burn:iteracoes]))
}

estat_burn_df <- data.frame(media=media_burn, mediana=mediana_burn, moda=moda_burn)
rownames(estat_burn_df) <- c(expression(beta[0]), expression(beta[1]),expression(beta[2]))

kable(estat_burn_df, caption = "Funções de perda dos betas após burn-in.") %>% 
  kable_styling(full_width = F)
```


## Ánalise de autocorrelação

```{r}

beta_burnin <- lapply(beta, function(x){x[I(burn+1):iteracoes]})

conf.level <- 0.95
ciline <- qnorm((1 - conf.level)/2)/sqrt(length(beta_burnin[[1]]))

bacf <- acf(beta_burnin[[1]], plot = FALSE, lag.max=2000)
bacfdf <- with(bacf, data.frame(lag, acf))

plot1 <- ggplot(data = bacfdf, mapping = aes(x = lag, y = acf)) +
       geom_hline(aes(yintercept = 0), color='slateblue') +
       geom_segment(mapping = aes(xend = lag, yend = 0), color='slateblue') + geom_hline(aes(yintercept = ciline), linetype = 3, color = 'darkred') + 
  geom_hline(aes(yintercept = -ciline), linetype = 3, color = 'darkred') +
  labs(y=expression(beta[0]))

bacf <- acf(beta_burnin[[2]], plot = FALSE, lag.max=2000)
bacfdf <- with(bacf, data.frame(lag, acf))

plot2 <- ggplot(data = bacfdf, mapping = aes(x = lag, y = acf)) +
       geom_segment(mapping = aes(xend = lag, yend = 0), color='#374d7c') + geom_hline(aes(yintercept = ciline), linetype = 3, color = 'darkred') + 
  geom_hline(aes(yintercept = -ciline), linetype = 3, color = 'darkred') +
  labs(y=expression(beta[1]))

bacf <- acf(beta_burnin[[3]], plot = FALSE, lag.max=2000)
bacfdf <- with(bacf, data.frame(lag, acf))

plot3 <- ggplot(data = bacfdf, mapping = aes(x = lag, y = acf)) +
       geom_segment(mapping = aes(xend = lag, yend = 0), color='#bf9000') + geom_hline(aes(yintercept = ciline), linetype = 3, color = 'darkred') + 
  geom_hline(aes(yintercept = -ciline), linetype = 3, color = 'darkred') +
  labs(y=expression(beta[2]))

grid.arrange(plot1, plot2, plot3, ncol=3)
```

## Autocorrelação após tratamento(thin)

```{r}
thin = 20
beta_thin <- lapply(beta_burnin, function(x){x[seq(1,length(x),by = thin)]})

par(mfrow = c(1,3))
acf(beta_thin[[1]],lag.max = 1000,col = "slateblue", main = expression(beta[0]))
acf(beta_thin[[2]],lag.max = 1000,col = "#374d7c", main = expression(beta[1]))
acf(beta_thin[[3]],lag.max = 1000,col = "#bf9000", main = expression(beta[2]))
```

## Cadeias Finais

```{r}
betas_thin <- data.frame(beta0=beta_thin[[1]], beta1=beta_thin[[2]], beta2=beta_thin[[3]])

plot1 <- betas_thin %>% 
  ggplot(aes(y=beta0, x=1:length(beta0))) + geom_line(color='slateblue') +
  labs(x="iterações", y=expression(beta[0]))

plot2 <- betas_thin %>% 
  ggplot(aes(y=beta0, x=1:length(beta1))) + geom_line(color='#374d7c') +
  labs(x="iterações", y=expression(beta[1]))

plot3 <- betas_thin %>% 
  ggplot(aes(y=beta0, x=1:length(beta2))) + geom_line(color='#bf9000') +
  labs(x="iterações", y=expression(beta[2]))


grid.arrange(plot1, plot2, plot3, ncol=3)
```

## Histogramas das distribuições a posteriori


```{r}
amostra_priori = rnorm(length(beta_thin[[1]]),0,sqrt(K))

par(mfrow = c(1,2))
hist(amostra_priori,prob = TRUE,ylab = "",xlab = "",main = expression(beta[0] ~ "a priori"),col = "#46edc8")
hist(beta_thin[[1]],prob = TRUE,ylab = "",xlab = "",main = expression(beta[0] ~ "a posteriori"),col = "#46edc8")
hist(amostra_priori,prob = TRUE,ylab = "",xlab = "",main = expression(beta[1] ~ "a priori"),col = "#374d7c")
hist(beta_thin[[2]],prob = TRUE,ylab = "",xlab = "",main = expression(beta[1] ~ "a posteriori"),col = "#374d7c")
hist(amostra_priori,prob = TRUE,ylab = "",xlab = "",main = expression(beta[2] ~ "a priori"),col = "#bf9000")
hist(beta_thin[[3]],prob = TRUE,ylab = "",xlab = "",main = expression(beta[2] ~ "a posteriori"),col = "#bf9000")
```

## Intervalo HPD

```{r}
perda_quadratica <- lapply(beta_thin, function(x){mean(x)})
perda_absoluta <- lapply(beta_thin, function(x){median(x)})
perda_zero_um <- lapply(beta_thin, function(x){moda(x)})

# alpha = 0.05
intervalo_credibilidade <- lapply(beta_thin, function(x){quantile(x, c(0.025,0.975))})
intervalo_HPD <- lapply(beta_thin, function(x){HPDinterval(as.mcmc(x))})

kable(data.frame(quadratica = unlist(perda_quadratica),
           absoluta = unlist(perda_absoluta),
           zero_um = unlist(perda_zero_um),
           HPD_inferior = unlist(lapply(intervalo_HPD, function(x){x[1]})),
           HPD_superior = unlist(lapply(intervalo_HPD, function(x){x[2]})),
           row.names = c(expression(beta[0]), expression(beta[1]),expression(beta[2])))
      ) %>% kable_styling(full_width = F)
```

## Histograma

Amostras da distribuição preditiva a posteriori de Y para valores das covariáveis X1 e X2.

```{r}
histograma_preditiva <- function(bx1, bx2, titulo){
  numerador <- exp(beta_thin[[1]]+beta_thin[[2]]*bx1+beta_thin[[3]]*bx2)
  denominador <- (1+exp(beta_thin[[1]]+beta_thin[[2]]*bx1+beta_thin[[3]]*bx2))
  prob <- numerador/denominador
  y_prev <- rbinom(iteracoes, 1, prob)
  summary(y_prev[burn:iteracoes])
  hist(y_prev[burn:iteracoes],
       prob=T,main=titulo,
       col=("#d48472"),xlab=" ")
}

par(mfrow = c(2,2))
histograma_preditiva(1, 0, "A") # "Preditiva para um não fumante que pratica atividades físicas")
histograma_preditiva(0, 0, "B") # "Preditiva para um não fumante que não pratica atividades físicas")
histograma_preditiva(1, 1, "C") # "Preditiva para um fumante que pratica atividades físicas")
histograma_preditiva(0, 1, "D") # "Preditiva para um fumante que não pratica atividades físicas")
```


## Comparando resultados com inferência clássica

```{r}
model_infclassica = glm(doenca_ou_infarto~ativ_fisica+fumante,family="binomial", data=heart_disease)
kable(data.frame(
  classico=coef(model_infclassica),
  bayesiano=c(mean(beta_thin[[1]]), mean(beta_thin[[2]]), mean(beta_thin[[3]])),
  row.names = (c("Intercepto", "Atividade Física", "Uso de cigarro"))
), caption = "Estimativas pontuais: clássicas versus bayesianas.") %>% 
  kable_styling(full_width = F)
```






